{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8701087",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import *\n",
    "import importlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, clear_output\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from torch import nn, Tensor\n",
    "from torch.distributions import Distribution, Dirichlet as TorchDirichlet\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from functools import reduce\n",
    "from torch.distributions import Bernoulli\n",
    "from plotting import make_vae_plots\n",
    "import plotting\n",
    "importlib.reload(plotting)\n",
    "import math \n",
    "import torch\n",
    "from torch.nn.functional import softplus\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e601961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train and test sets\n",
    "dset_train = MNIST(\"./\", train=True,  transform=ToTensor(), download=True)\n",
    "dset_test  = MNIST(\"./\", train=False, transform=ToTensor())\n",
    "\n",
    "# The digit classes to use\n",
    "classes = [3, 7]\n",
    "\n",
    "def stratified_sampler(labels):\n",
    "    \"\"\"Sampler that only picks datapoints corresponding to the specified classes\"\"\"\n",
    "    (indices,) = np.where(reduce(lambda x, y: x | y, [labels.numpy() == i for i in classes]))\n",
    "    indices = torch.from_numpy(indices)\n",
    "    return SubsetRandomSampler(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9959a256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cahar\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:65: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "c:\\Users\\cahar\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:70: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 64\n",
    "eval_batch_size = 100\n",
    "# The loaders perform the actual work\n",
    "train_loader = DataLoader(dset_train, batch_size=batch_size,\n",
    "                          sampler=stratified_sampler(dset_train.train_labels))\n",
    "test_loader  = DataLoader(dset_test, batch_size=eval_batch_size, \n",
    "                          sampler=stratified_sampler(dset_test.test_labels))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "def reduce(x: Tensor) -> Tensor:\n",
    "    \"\"\"Reduce only if tensor has more than one dimension.\"\"\"\n",
    "    return x if x.ndim == 1 else x.view(x.size(0), -1).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3871e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VariationalInference(nn.Module):\n",
    "    def __init__(self, beta: float = 1.):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, model: nn.Module, x: Tensor):\n",
    "        # forward pass through the model\n",
    "        outputs = model(x)\n",
    "        \n",
    "        # unpack outputs\n",
    "        px, pz, qz, z = [outputs[k] for k in [\"px\", \"pz\", \"qz\", \"z\"]]\n",
    "        \n",
    "        # evaluate log probabilities\n",
    "        log_px = reduce(px.log_prob(x))\n",
    "        log_pz = reduce(pz.log_prob(z))\n",
    "        log_qz = reduce(qz.log_prob(z))\n",
    "        \n",
    "        # compute the KL divergence term\n",
    "        kl = log_qz - log_pz\n",
    "        \n",
    "        # ELBO: log p(x|z) - KL(q||p)\n",
    "        elbo = log_px - kl\n",
    "        \n",
    "        # β-ELBO: log p(x|z) - β * KL(q||p)\n",
    "        beta_elbo = log_px - self.beta * kl\n",
    "        \n",
    "        # loss = -E_q[Lβ]\n",
    "        loss = -beta_elbo.mean()\n",
    "        \n",
    "        # diagnostics for monitoring\n",
    "        with torch.no_grad():\n",
    "            diagnostics = {'elbo': elbo, 'log_px': log_px, 'kl': kl}\n",
    "            \n",
    "        return loss, diagnostics, outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc07712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.functional.regression import mean_squared_error, mean_absolute_error\n",
    "from torchmetrics.functional.image.ssim import structural_similarity_index_measure\n",
    "from models.convolutional_vae import VariationalAutoencoder\n",
    "def compute_reconstruction_errors(x_true: torch.Tensor, x_recon: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute standard reconstruction error metrics using torchmetrics.\n",
    "\n",
    "    Args:\n",
    "        x_true: Ground-truth tensor of shape [B, C, H, W].\n",
    "        x_recon: Reconstructed tensor of same shape.\n",
    "\n",
    "    Returns:\n",
    "        dict with keys: 'MSE', 'RMSE', 'MAE', 'PSNR', 'SSIM'\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure same dtype and device\n",
    "    x_true = x_true.to(x_recon.device, dtype=x_recon.dtype)\n",
    "\n",
    "    # Ensure same shape\n",
    "    assert x_true.shape == x_recon.shape, \"x_true and x_recon must have the same shape\"\n",
    "\n",
    "    # Compute metrics\n",
    "    mse = mean_squared_error(x_recon, x_true).item()\n",
    "    rmse = mse ** 0.5\n",
    "    mae = mean_absolute_error(x_recon, x_true).item()\n",
    "    ssim = structural_similarity_index_measure(x_recon, x_true, data_range=1.0).item()\n",
    "\n",
    "    return {\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"SSIM\": ssim,\n",
    "    }\n",
    "\n",
    "def plot_reconstructions_with_metrics(vae: VariationalAutoencoder, test_loader: DataLoader, device: torch.device):\n",
    "    with torch.no_grad():\n",
    "        vae.eval()\n",
    "        x, y = next(iter(test_loader))\n",
    "        x = x.to(device)\n",
    "        outputs = vae(x)\n",
    "\n",
    "        # Plot reconstructions with their metrics\n",
    "        num_images = 8\n",
    "        fig, axes = plt.subplots(3, num_images, figsize=(num_images * 2, 6))\n",
    "        plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "        for i in range(num_images):\n",
    "            # Compute metrics for each image\n",
    "            diagnostics = compute_reconstruction_errors(\n",
    "                x[i].unsqueeze(0), outputs['px'].mean[i].unsqueeze(0)\n",
    "            )\n",
    "\n",
    "            # --- Original ---\n",
    "            axes[0, i].imshow(x[i].cpu().squeeze(), cmap='gray')\n",
    "            axes[0, i].set_title(f\"Image {i+1}\", fontsize=10)\n",
    "            axes[0, i].axis('off')\n",
    "\n",
    "            # --- Reconstruction ---\n",
    "            axes[1, i].imshow(outputs['px'].mean[i].cpu().squeeze(), cmap='gray')\n",
    "            axes[1, i].axis('off')\n",
    "\n",
    "            # --- Metrics ---\n",
    "            metrics_text = \"\\n\".join([f\"{k}: {v:.4f}\" for k, v in diagnostics.items()])\n",
    "            axes[2, i].text(\n",
    "                0.5, 0.5, metrics_text,\n",
    "                color='black',\n",
    "                fontsize=16,\n",
    "                ha='center', va='center',\n",
    "                family='monospace'\n",
    "            )\n",
    "            axes[2, i].axis('off')\n",
    "\n",
    "        axes[0, 0].set_ylabel(\"Original\", fontsize=12)\n",
    "        axes[1, 0].set_ylabel(\"Reconstruction\", fontsize=12)\n",
    "        axes[2, 0].set_ylabel(\"Metrics\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59369445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cahar\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from utils import LatentType\n",
    "from models.convolutional_vae import VariationalAutoencoder\n",
    "\n",
    "# define the models, evaluator and optimizer\n",
    "# VAE\n",
    "latent_features = 2  # can always be changed\n",
    "vae = VariationalAutoencoder(images[0].shape, latent_features, latent_type = LatentType.DIRICHLET)\n",
    "\n",
    "# Evaluator: Variational Inference\n",
    "beta = 1\n",
    "vi = VariationalInference(beta=beta)\n",
    "\n",
    "# The Adam optimizer works really well with VAEs.\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "# define dictionary to store the training curves\n",
    "training_data = defaultdict(list)\n",
    "validation_data = defaultdict(list)\n",
    "\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786c8dbe",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Pull and update code (create utils class)\n",
    "- Update T-SNE diagram to work with x dimensional latent spaces\n",
    "- Create one-hot-encoding reconstructions with latent variables like the dirichlet paper\n",
    "- Save and loading of models\n",
    "- Explore UMAP and MDS more\n",
    "- Look more at greyscale transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beb9031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Using device: cpu\n",
      "epoch: 1\n",
      "elbo: -161.8885040283203\n",
      "log px: -159.9197998046875\n",
      "kl-divergence: 1.9687070846557617\n",
      "epoch: 2\n",
      "elbo: -173.84388732910156\n",
      "log px: -170.15965270996094\n",
      "kl-divergence: 3.6842355728149414\n",
      "epoch: 3\n",
      "elbo: -160.43516540527344\n",
      "log px: -158.02325439453125\n",
      "kl-divergence: 2.411924362182617\n",
      "epoch: 4\n",
      "elbo: -168.97825622558594\n",
      "log px: -166.19334411621094\n",
      "kl-divergence: 2.784891128540039\n",
      "epoch: 5\n",
      "elbo: -166.98187255859375\n",
      "log px: -164.47889709472656\n",
      "kl-divergence: 2.5029830932617188\n",
      "epoch: 6\n",
      "elbo: -152.03614807128906\n",
      "log px: -149.53038024902344\n",
      "kl-divergence: 2.505770683288574\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "from plotting import make_vae_plots\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\">> Using device: {device}\")\n",
    "\n",
    "vae = vae.to(device)\n",
    "\n",
    "\n",
    "# annealing parameters\n",
    "tau_min = 0.5\n",
    "anneal_rate = 0.0003\n",
    "kl_warmup_epochs = 20  # number of epochs to reach full β\n",
    "max_beta = 0.3\n",
    "num_batches = len(train_loader)\n",
    "\n",
    "while epoch < num_epochs:\n",
    "    epoch += 1\n",
    "    training_epoch_data = defaultdict(list)\n",
    "    vae.train()\n",
    "\n",
    "    # KL warmup: gradually increase β\n",
    "    vi.beta = min(max_beta, epoch / kl_warmup_epochs)\n",
    "    \n",
    "    # training loop\n",
    "    for batch_idx, (x, y) in  enumerate(train_loader, start=1):\n",
    "        if batch_idx == num_batches:\n",
    "              print(\"epoch:\",epoch)\n",
    "              print(\"elbo:\",round(diagnostics[\"elbo\"].mean().item()),2)\n",
    "              print(\"log px:\",round(diagnostics[\"log_px\"].mean().item()),2)\n",
    "              print(\"kl-divergence:\",round(diagnostics[\"kl\"].mean().item()),2)\n",
    "\n",
    "        x = x.to(device)\n",
    "        loss, diagnostics, outputs = vi(vae, x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        for k, v in diagnostics.items():\n",
    "            training_epoch_data[k] += [v.mean().item()]\n",
    "\n",
    "    for k, v in training_epoch_data.items():\n",
    "        training_data[k] += [np.mean(training_epoch_data[k])]\n",
    "\n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        vae.eval()\n",
    "        x, y = next(iter(test_loader))\n",
    "        x = x.to(device)\n",
    "        loss, diagnostics, outputs = vi(vae, x)\n",
    "        for k, v in diagnostics.items():\n",
    "            validation_data[k] += [v.mean().item()]\n",
    "\n",
    "    # visualize\n",
    "    # make_vae_plots(vae, x, y, outputs, training_data, validation_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f57d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import latent_morphing\n",
    "plot_reconstructions_with_metrics(vae, train_loader, device=device)\n",
    "#latent_morphing(vae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
